curr_b <- pt_estim_mean - (pos_data - osm_data)
(indiv_bias <- curr_b)
}
print(indiv_bias)
print(mean(indiv_bias))
curr_b <- pt_estim_mean - mean(pos_data - osm_data)
german_data <- read.csv("germancredit.csv")
german_data <- read.csv("germancredit.csv")
View(german_data)
View(german_data)
german_data <- read.csv("germancredit.csv", stringsAsFactors = T)
View(german_data)
# Produce a logistic regression model using all the predictors
all_lr <- glm(Default ~ ., data = german_data, family = "binomial")
summary(all_lr)
#2c. Use package to estimate LOOCV for full log-reg model
cost <- function(r, pi = 0){mean(abs(r - pi) > 0.5)}
all_lr.err <- cv.glm(german_data, all_lr, cost, K=nrow(german_data))
all_lr.err$delta
library(boot)
o2_data <- read.table("oxygen_saturation.txt", header = T, sep="\t")
pos_data <- o2_data[, 1]
osm_data <- o2_data[, 2]
hist(pos_data)
hist(osm_data)
mean(pos_data)
mean(osm_data)
pt_estim_mean <- mean(pos_data) - mean(osm_data)
biases <- 0
for(i in 1:72){
curr_b <- (pos_data[i] - osm_data[i]) - pt_estim_mean
biases <- biases + curr_b
}
biases / 72
SD(pos_data) / sqrt(72)
?SD
sd(pos_data) / sqrt(72)
sd(osm_data) / sqrt(72)
sd(pos_data)
sd(osm_data)
sd(pos_data) / sqrt(72)
sd(osm_data) / sqrt(72)
sd(o2_data) / sqrt(72)
sd(as.matrix(o2_data)) / sqrt(72)
n <- nrow(o2_data)
indices_sampled <- sample(1:n, n, replace = T)
sample(1:n, n, replace = T)
b <- 1000
pos_data[indices_sampled]
osm_data[indices_sampled]
mean(pos_data[indices_sampled]) - mean(osm_data[indices_sampled])
estimates <- c()
for(i in 1:b){
indices_sampled <- sample(1:n, n, replace = T)
estimates <- c(estimates,
mean(pos_data[indices_sampled]) - mean(osm_data[indices_sampled])
)
}
estimates
mean(estimates)
var(estimates)
mean(estimates) - pt_estim_mean
sd(estimates)
quantile(estimates, c(.025, .975))
mean.fn <- function(x, indices) {
result <- mean(x[indices,1]) - mean(x[indices,2])
return(result)
}
mean.boot <- boot(o2_data, mean.fn, R = 1000)
mean.boot$t0
mean(mean.boot$t) - mean.boot$t0
sd(mean.boot$t)
boot.ci(mean.boot, type = "perc")
View(o2_data)
diff_data <- pos_data - osm_data
#sd(pos_data) / sqrt(72)
#sd(osm_data) / sqrt(72)
# Bias
mean(diff_data) - pt_estim_mean
sd(diff_data) / sqrt(72)
quantile(diff_data, c(0.025, 0.975))
st_err <- sd(diff_data) / sqrt(72)
mean(diff_data) + st_err
mean(diff_data) - st_err
mean(diff_data)
quantile(diff_data, c(0.025, 0.975))
length(estimates)
# Standard Error
sd(estimates) / sqrt(1000)
length(mean.boot$t)
mean.boot <- boot(o2_data, mean.fn, R = 1000)
mean.boot
mean.boot$t0
#
mean(mean.boot$t) - mean.boot$t0
#
sd(mean.boot$t) / sqrt(1000)
# Confidence intervals produced
boot.ci(mean.boot, type = "perc")
library(MASS)
german_data <- read.csv("germancredit.csv", stringsAsFactors = T)
# Produce a logistic regression model using all the predictors
all_lr <- glm(Default ~ ., data = german_data, family = "binomial")
summary(all_lr)
#2c. Use package to estimate LOOCV for full log-reg model
cost <- function(r, pi = 0){mean(abs(r - pi) > 0.5)}
all_lr.err <- cv.glm(german_data, all_lr, cost, K=nrow(german_data))
all_lr.err$delta
library(bestGLM)
install.packages("bestglm")
library(bestglm)
View(german_data)
library(glmulti)
install.packages("glmulti")
?glmulti
library(glmulti)
?glmulti
best_sub <- glmulti(Default ~ ., data=german_data, level=1,
method="h", crit="aic", confsetsize = 2,
plotty=F, report=F,
fitfunction = "glm", family=binomial)
library(MASS)
library(bestglm)
library(glmulti)
german_data <- read.csv("germancredit.csv", stringsAsFactors = T)
# Produce a logistic regression model using all the predictors
all_lr <- glm(Default ~ ., data = german_data, family = "binomial")
summary(all_lr)
#2c. Use package to estimate LOOCV for full log-reg model
cost <- function(r, pi = 0){mean(abs(r - pi) > 0.5)}
all_lr.err <- cv.glm(german_data, all_lr, cost, K=nrow(german_data))
library(boot)
all_lr.err <- cv.glm(german_data, all_lr, cost, K=nrow(german_data))
all_lr.err$delta
?stepAIC
empty_lr <- glm(Default ~ NULL, data=germand_data, family = "binomial")
empty_lr <- glm(Default ~ NULL, data=german_data, family = "binomial")
summary(empty_lr)
?cv.glm
all_lr.err$delta
empty_lr.err <- cv.glm(german_data, empty_lr, cost, K=nrow(german_data))
empty_lr.err$delta
?stepAIC
empty_lr <- glm(Default ~ 1, data=german_data, family = "binomial")
summary(empty_lr)
empty_lr.err <- cv.glm(german_data, empty_lr, cost, K=nrow(german_data))
empty_lr.err$delta
forward_step <- stepAIC(empty_lr,
scope = list(lower=empty_lr, upper=all_lr),
direction = "forward")
library(boot)
library(MASS)
forward_step <- stepAIC(empty_lr,
scope = list(lower=empty_lr, upper=all_lr),
direction = "forward")
german_data <- read.csv("germancredit.csv", stringsAsFactors = T)
# Produce a logistic regression model using all the predictors
all_lr <- glm(Default ~ ., data = german_data, family = "binomial")
summary(all_lr)
empty_lr <- glm(Default ~ 1, data=german_data, family = "binomial")
summary(empty_lr)
#2c. Use package to estimate LOOCV for full log-reg model
cost <- function(r, pi = 0){mean(abs(r - pi) > 0.5)}
forward_step <- stepAIC(empty_lr,
scope = list(lower=empty_lr, upper=all_lr),
direction = "forward")
forward_lr <- glm(Default ~ checkingstatus1 + duration + history + purpose + savings +
others + installment + status + amount + otherplans + foreign +
age + housing + tele, data = german_data, family="binomial")
summary(forward_lr)
forward_lr.err <- cv.glm(german_data, forward_lr, cost, K=nrow(german_data))
forward_lr.err$delta
library(boot)
library(MASS)
library(bestglm)
german_data <- read.csv("germancredit.csv", stringsAsFactors = T)
# Produce a logistic regression model using all the predictors
all_lr <- glm(Default ~ ., data = german_data, family = "binomial")
summary(all_lr)
empty_lr <- glm(Default ~ 1, data=german_data, family = "binomial")
summary(empty_lr)
#2c. Use package to estimate LOOCV for full log-reg model
cost <- function(r, pi = 0){mean(abs(r - pi) > 0.5)}
forward_step <- stepAIC(empty_lr,
scope = list(lower=empty_lr, upper=all_lr),
direction = "forward")
forward_lr <- glm(Default ~ checkingstatus1 + duration + history + purpose + savings +
others + installment + status + amount + otherplans + foreign +
age + housing + tele, data = german_data, family="binomial")
summary(forward_lr)
forward_lr.err <- cv.glm(german_data, forward_lr, cost, K=nrow(german_data))
forward_lr.err$delta
backward_step <- stepAIC(all_lr,
scope = list(lower=empty_lr, upper=all_lr),
direction = "backward")
backward_lr <- glm(Default ~ checkingstatus1 + duration + history + purpose + amount +
savings + installment + status + others + age + otherplans +
housing + tele + foreign, data = german_data, family = "binomial")
summary(backward_lr)
backward_lr.err <- cv.glm(german_data, backward_lr, cost, K=nrow(german_data))
backward_lr.err$delta
library(glmulti)
best_sub <- glmulti(Default ~ checkingstatus1 + duration + history + purpose + savings +
others + installment + status + amount + otherplans + foreign +
age + housing + tele, data=german_data, level=1,
method="h", crit="aic", confsetsize = 2,
plotty=F, report=F,
fitfunction = "glm", family=binomial)
best_sub
View(forward_step)
best_sub@objects[[1]]
best_sub@formulas[[1]]
best_lr <- glm(best_sub@formulas[[1]], data=german_data, family="binomial")
summary(best_lr)
best_lr.err <- cv.glm(german_data, best_lr, cost, K=nrow(german_data))
best_lr.err$delta
?model.matrix
model.matrix(Default ~ ., german_data)
model.matrix(Default ~ ., german_data)[, -1]
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0)
library(glmnet)
?glmnet
cv_ridge <- cv.glmnet(X, y, alpha = 0)
y <- german_data$Default
X <- model.matrix(Default ~ ., german_data)[, -1]
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0)
cv_ridge <- cv.glmnet(X, y, alpha = 0)
best_lambda <- cv_ridge$lambda.min
best_lambda
best_ridgemod <- glmnet(X, y, alpha = 0, lambda = best_lambda)
coef(best_model)
coef(best_ridgemod)
y_pred <- predict(ridge.mod, s = best_lambda, newx = X)
(y_pred - y)^2
mean((y_pred - y)^2)
y_pred <- predict(best_ridgemod, s = best_lambda, newx = X)
mean((y_pred - y)^2)
cv_lasso <- cv.glmnet(X, y, alpha=1)
lasso.mod <- glmnet(X, y, alpha=1)
best_lasso_lambda <- cv_lasso$lambda.min
best_lasso_lambda
best_lassomod <- glmnet(X, y, alpha=1, lambda=best_lasso_lambda)
coef(best_lassomod)
y_pred <- predict(lasso.mod, s = best_lasso_lambda, newx = X)
mean((y_pred-y)^2)
nrow(o2_data)
library(boot)
# Read in prostate cancer data
pc_data <- read.csv("prostate_cancer.csv")
# Eliminate subject number feature
pc_data <- pc_data[,-1]
# Convert gleason and treat vesinv as qualitative variables
pc_data$vesinv <- factor(pc_data$vesinv, order=F, levels = c(0, 1))
pc_data$gleason <- factor(pc_data$gleason, order=F, levels = c(6, 7, 8))
pc_data[, 1] <- log(pc_data[, 1])
hist(pc_data[, 1])
summary(glm(psa ~ ., data = pc_data))
View(pc_data)
# Read in prostate cancer data
pc_data <- read.csv("prostate_cancer.csv")
# Eliminate subject number feature
pc_data <- pc_data[,-1]
# Convert gleason and treat vesinv as qualitative variables
pc_data$vesinv <- factor(pc_data$vesinv, order=F, levels = c(0, 1))
pc_data[, 1] <- log(pc_data[, 1])
hist(pc_data[, 1])
summary(glm(psa ~ ., data = pc_data))
# Convert gleason and treat vesinv as qualitative variables
pc_data$vesinv <- factor(pc_data$vesinv, order=F, levels = c(0, 1))
pc_data[, 1] <- log(pc_data[, 1])
hist(pc_data[, 1])
summary(glm(psa ~ ., data = pc_data))
library(boot)
# Read in prostate cancer data
pc_data <- read.csv("prostate_cancer.csv")
# Eliminate subject number feature
pc_data <- pc_data[,-1]
# Convert gleason and treat vesinv as qualitative variables
pc_data$vesinv <- factor(pc_data$vesinv, order=F, levels = c(0, 1))
pc_data[, 1] <- log(pc_data[, 1])
hist(pc_data[, 1])
summary(glm(psa ~ ., data = pc_data))
regfit_full = regsubsets(psa ~ ., data = pc_data)
library(leaps)
regfit_full = regsubsets(psa ~ ., data = pc_data)
summary(regfit_full)
regfit_full = regsubsets(psa ~ ., data = pc_data, nvmax = 7)
summary(regfit_full)
names(summary(regfit_full))
full_model <- glm(psa ~ ., data = pc_data)
cv.err <- cv.glm(pc_data, full_model)
cv.err$delta
regfit_full = regsubsets(psa ~ ., data = pc_data, nvmax = 7)
regfit_summ <- summary(regfit_full)
which.max(regfit_summ$adjr2)
coef(regfit_full, 4)
regfit_full$formula
regfit_summ$adjr2
coef(regfit_full, 4)
cv.glm(pc_data, coef(regfit_full, 4))
regfit_summ$which
coef(regfit_full, 4)
glm(psa ~ cancervol + benpros + vesinv + gleason, data=psa_data)
glm(psa ~ cancervol + benpros + vesinv + gleason, data=pc_data)
cv.glm(pc_data, glm(psa ~ cancervol + benpros + vesinv + gleason, data=pc_data))$delta
fit.fwd = regsubsets(psa ~ ., data = pc_data, nvmax = 7, method = "forward")
summary(fit.fwd)
which.max(summary(fit.fwd)$adjr2)
coef(fit.fwd, 4)
cv.glm(pc_data, glm(psa ~ cancervol + benpros + vesinv + gleason, data=pc_data))$delta
fit.bwd = regsubsets(psa ~ ., data = pc_data, nvmax = 7, method = "backward")
summary(fit.bwd)
which.max(summary(fit.bwd)$adjr2)
coef(fit.bwd, 4)
cv.glm(pc_data, glm(psa ~ cancervol + benpros + vesinv + gleason, data=pc_data))$delta
y <- pc_data$psa
X <- model.matrix(psa ~ ., pc_data)[, -1]
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0)
library(glmnet)
y <- pc_data$psa
X <- model.matrix(psa ~ ., pc_data)[, -1]
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0)
cv_ridge <- cv.glmnet(X, y, alpha = 0)
best_lambda <- cv_ridge$lambda.min
best_lambda
best_ridgemod <- glmnet(X, y, alpha = 0, lambda = best_lambda)
coef(best_ridgemod)
y_pred <- predict(ridge.mod, s = best_lambda, newx = X)
mean((y_pred - y)^2)
lasso.mod <- glmnet(X, y, alpha=1)
cv_lasso <- cv.glmnet(X, y, alpha=1)
best_lasso_lambda <- cv_lasso$lambda.min
best_lasso_lambda
best_lassomod <- glmnet(X, y, alpha=1, lambda=best_lasso_lambda)
coef(best_lassomod)
y_pred <- predict(lasso.mod, s = best_lasso_lambda, newx = X)
mean((y_pred-y)^2)
?dbinom
pbinom(2, 10, 0.3)
pbinom(2, 10, 0.03)
pbinom(1, 10, 0.03, lower.tail = F)
pbinom(1, 100, 0.03, lower.tail = F)
pbinom(1, 10, 0.03, lower.tail = F)
pbinom(1, 50, 0.03, lower.tail = F)
dbinom(1, 50, 0.03, lower.tail = F)
dbinom(1, 50, 0.03)
library(boot)
library(leaps)
library(glmnet)
# Read in prostate cancer data
pc_data <- read.csv("prostate_cancer.csv")
# Eliminate subject number feature
pc_data <- pc_data[,-1]
# Treat vesinv as a qualitative variable
pc_data$vesinv <- factor(pc_data$vesinv, order=F, levels = c(0, 1))
# Conduct a natural log transformation on the response
# to adjust it's distribution to something more appropriate.
pc_data[, 1] <- log(pc_data[, 1])
hist(pc_data[, 1])
# Make a full model of a linear regression with psa as response and all features as predictors
# Calculate test MSE via LOOCV
full_model <- glm(psa ~ ., data = pc_data)
cv.err <- cv.glm(pc_data, full_model)
cv.err$delta
# Find a reasonable subset of features to implement a linear regression model with
# via the best-subset selection accounting for the best adjusted R^2.
regfit_full = regsubsets(psa ~ ., data = pc_data, nvmax = 7)
regfit_summ <- summary(regfit_full)
which.max(regfit_summ$adjr2)
coef(regfit_full, 4)
glm(psa ~ cancervol + benpros + vesinv + gleason, data=pc_data)
# Find the test MSE via LOOCV
cv.glm(pc_data, glm(psa ~ cancervol + benpros + vesinv + gleason, data=pc_data))$delta
# Make a full model of a linear regression with psa as response and all features as predictors
# Calculate test MSE via LOOCV
full_model <- glm(psa ~ ., data = pc_data)
full_model
# Find a reasonable subset of features to implement a model with using forward
# subset selection with best adjusted R^2 value.
fit.fwd = regsubsets(psa ~ ., data = pc_data, nvmax = 7, method = "forward")
summary(fit.fwd)
which.max(summary(fit.fwd)$adjr2)
coef(fit.fwd, 4)
# Find a reasonable subset of features to implement a model with using backward
# subset selection with best adjusted R^2 value.
fit.bwd = regsubsets(psa ~ ., data = pc_data, nvmax = 7, method = "backward")
summary(fit.bwd)
which.max(summary(fit.bwd)$adjr2)
coef(fit.bwd, 4)
# Select response and feature data as y and X respectively
y <- pc_data$psa
X <- model.matrix(psa ~ ., pc_data)[, -1]
# Set up a grid of potential lambda values
grid <- 10^seq(10, -2, length = 100)
# Using alpha = 0, conduct a ridge regression
ridge.mod <- glmnet(X, y, alpha = 0)
# Use LOOCV to determine the best penalty parameter
cv_ridge <- cv.glmnet(X, y, alpha = 0)
best_ridge_lambda <- cv_ridge$lambda.min
best_ridge_lambda
# Use the best lambda to find the best ridge regression model
best_ridgemod <- glmnet(X, y, alpha = 0, lambda = best_ridge_lambda)
coef(best_ridgemod)
fit.bwd
summary(fit.bwd)
summary(fit.bwd)[4]
# Using alpha=1, conduct lasso
lasso.mod <- glmnet(X, y, alpha=1)
# Use LOOCV to find the best penalty parameter
cv_lasso <- cv.glmnet(X, y, alpha=1)
best_lasso_lambda <- cv_lasso$lambda.min
best_lasso_lambda
# Make the best lasso model with the best lambda value
best_lassomod <- glmnet(X, y, alpha=1, lambda=best_lasso_lambda)
coef(best_lassomod)
library(boot)
library(MASS)
library(glmulti)
library(glmnet)
# Read in german credit dataset and store as a dataframe
german_data <- read.csv("germancredit.csv", stringsAsFactors = T)
# Produce a logistic regression model using all the predictors
all_lr <- glm(Default ~ ., data = german_data, family = "binomial")
summary(all_lr)
summary(empty_lr)
# Use package to estimate LOOCV for log-reg models
cost <- function(r, pi = 0){mean(abs(r - pi) > 0.5)}
# Calculate LOOCV for both the full model and the null model
all_lr.err <- cv.glm(german_data, all_lr, cost, K=nrow(german_data))
all_lr.err$delta
empty_lr.err <- cv.glm(german_data, empty_lr, cost, K=nrow(german_data))
# Produce a logistic regression model using no predictors (yields intercept only model)
empty_lr <- glm(Default ~ 1, data=german_data, family = "binomial")
summary(empty_lr)
empty_lr.err <- cv.glm(german_data, empty_lr, cost, K=nrow(german_data))
empty_lr.err$delta
all_lr.err$delta
best_sub <- glmulti(Default ~ checkingstatus1 + duration + history + purpose + savings +
others + installment + status + amount + otherplans + foreign +
age + housing + tele, data=german_data, level=1,
method="h", crit="aic", confsetsize = 2,
plotty=F, report=F,
fitfunction = "glm", family=binomial)
coef(all_lr)
best_lr <- glm(best_sub@formulas[[1]], data=german_data, family="binomial")
summary(best_lr)
coef(best_lr)
best_lr.err <- cv.glm(german_data, best_lr, cost, K=nrow(german_data))
best_lr.err$delta
forward_step <- stepAIC(empty_lr,
scope = list(lower=empty_lr, upper=all_lr),
direction = "forward")
# Feed the formula found into the glm function and conduct a logistic regression
forward_lr <- glm(Default ~ checkingstatus1 + duration + history + purpose + savings +
others + installment + status + amount + otherplans + foreign +
age + housing + tele, data = german_data, family="binomial")
summary(forward_lr)
coef(forward_lr)
forward_lr.err <- cv.glm(german_data, forward_lr, cost, K=nrow(german_data))
forward_lr.err$delta
# Find backward stepwise selection model with AIC as our method of evaluation
# I started with the full model and moved backward with the specified scope
backward_step <- stepAIC(all_lr,
scope = list(lower=empty_lr, upper=all_lr),
direction = "backward")
# Feed the formula found into the glm function and conduct a logistic regression
backward_lr <- glm(Default ~ checkingstatus1 + duration + history + purpose + amount +
savings + installment + status + others + age + otherplans +
housing + tele + foreign, data = german_data, family = "binomial")
summary(backward_lr)
coef(backward_lr)
backward_lr.err <- cv.glm(german_data, backward_lr, cost, K=nrow(german_data))
backward_lr.err$delta
y <- german_data$Default
X <- model.matrix(Default ~ ., german_data)[, -1]
grid <- 10^seq(10, -2, length = 100)
y
# Conduct a ridge regression where alpha=0
ridge.mod <- glmnet(X, y, alpha = 0)
# Use LOOCV to determine the best penalty parameter
cv_ridge <- cv.glmnet(X, y, alpha = 0)
best_ridge_lambda <- cv_ridge$lambda.min
best_ridge_lambda
# Make the best ridge regression model using the best lambda
best_ridgemod <- glmnet(X, y, alpha = 0, lambda = best_ridge_lambda)
coef(best_ridgemod)
as.matrix(coef(best_ridgemod), ncol=3)
as.matrix(coef(best_ridgemod), ncol=5)
?as.matrix
as.matrix(coef(best_ridgemod), nrow=10, ncol=5)
coef(best_ridgemod)
# Predict the values of the training data using the best ridge regression model
y_pred <- predict(ridge.mod, s = best_ridge_lambda, newx = X)
y_pred
# Compare the values to the true values to find test error
mean((y_pred - y)^2)
# Conduct a lasso model with alpha=1
lasso.mod <- glmnet(X, y, alpha=1)
# Use LOOCV to find the best lambda
cv_lasso <- cv.glmnet(X, y, alpha=1)
best_lasso_lambda <- cv_lasso$lambda.min
best_lasso_lambda
# Formulate the best lasso model with the best lasso lambda
best_lassomod <- glmnet(X, y, alpha=1, lambda=best_lasso_lambda)
coef(best_lassomod)
# Predict the values of our training data and compare to the true values to find the test error
y_pred <- predict(lasso.mod, s = best_lasso_lambda, newx = X)
mean((y_pred-y)^2)
