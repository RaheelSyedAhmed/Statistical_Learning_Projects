k <- nrow(pc_data)
indices <- sample(1:nrow(pc_data))
folds <- cut(indices, breaks = k, labels = FALSE)
unpruned_MSEs <- c()
pruned_MSEs <- c()
bagged_MSEs <- c()
rf_MSEs <- c()
boost_MSEs <- c()
for (i in 1:k){
val_indices <- which(folds == i, arr.ind = TRUE)
val_data <- pc_data[val_indices,]
train_data <- pc_data[-val_indices,]
train_tree_pc <- tree(psa ~ ., train_data)
unpruned_MSE <- (val_data$psa - predict(train_tree_pc, val_data))^2
unpruned_MSEs <- c(unpruned_MSEs, unpruned_MSE)
train_pruned_tree <- prune.tree(train_tree_pc, best=4)
pruned_MSE <- (val_data$psa - predict(train_pruned_tree, val_data))^2
pruned_MSEs <- c(pruned_MSEs, pruned_MSE)
train_bag <- randomForest(psa ~ ., data = train_data, mtry = 7, ntree = 1000, importance = TRUE)
bagged_MSE <- (val_data$psa - predict(train_bag, newdata = val_data))^2
bagged_MSEs <- c(bagged_MSEs, bagged_MSE)
train_rf <- randomForest(psa ~ ., data = train_data, mtry = round(7/3), ntree = 1000, importance = TRUE)
rf_MSE <- (val_data$psa - predict(train_rf, newdata = val_data))^2
rf_MSEs <- c(rf_MSEs, rf_MSE)
train_boost <- gbm(psa ~ ., data = train_data, distribution = "gaussian",
n.trees = 1000, interaction.depth = 1, shrinkage=0.01)
boost_MSE <- (val_data$psa - predict(train_boost, newdata = val_data, n.trees = 1000))^2
boost_MSEs <- c(boost_MSEs, boost_MSE)
}
result <- c(
mean(unpruned_MSEs),
mean(pruned_MSEs),
mean(bagged_MSEs),
mean(rf_MSEs),
mean(boost_MSEs)
)
return(setNames(result, c("unpruned_est_MSE", "pruned_est_MSE", "bagged_est_MSE", "rand_forest_est_MSE", "boosted_est_MSE")))
}
MSEs <- LOOCV_tree()
MSEs
# d
rf.pc <- randomForest(psa ~ ., data = pc_data, mtry = round(7/3), ntree = 1000, importance = TRUE)
rf.pc
importance(rf.pc)
varImpPlot(rf.pc)
?importance
plot(importance(rf.pc))
varImpPlot(rf.pc)
importance(rf.pc, type = 1)
plot(importance(rf.pc, type=1))
library(reticulate)
use_virtualenv(virtualenv="r-reticulate", required=TRUE)
library(keras)
library(tree)
library(randomForest)
library(gbm)
# Read in prostate cancer data
pc_data <- read.csv("prostate_cancer.csv")
# Eliminate subject number feature
pc_data <- pc_data[,-1]
# Treat vesinv as a qualitative variable
pc_data$vesinv <- factor(pc_data$vesinv, order=F, levels = c(0, 1))
# Conduct a natural log transformation on the response
# to adjust it's distribution to something more appropriate.
pc_data[, 1] <- log(pc_data[, 1])
hist(pc_data[, 1])
#a
tree_pc <- tree(psa ~ ., pc_data)
summary(tree_pc)
plot(tree_pc)
text(tree_pc, pretty = 0, cex = 0.7)
LOOCV_tree <- function(){
k <- nrow(pc_data)
indices <- sample(1:nrow(pc_data))
folds <- cut(indices, breaks = k, labels = FALSE)
unpruned_MSEs <- c()
pruned_MSEs <- c()
bagged_MSEs <- c()
rf_MSEs <- c()
boost_MSEs <- c()
for (i in 1:k){
val_indices <- which(folds == i, arr.ind = TRUE)
val_data <- pc_data[val_indices,]
train_data <- pc_data[-val_indices,]
train_tree_pc <- tree(psa ~ ., train_data)
unpruned_MSE <- (val_data$psa - predict(train_tree_pc, val_data))^2
unpruned_MSEs <- c(unpruned_MSEs, unpruned_MSE)
train_pruned_tree <- prune.tree(train_tree_pc, best=4)
pruned_MSE <- (val_data$psa - predict(train_pruned_tree, val_data))^2
pruned_MSEs <- c(pruned_MSEs, pruned_MSE)
train_bag <- randomForest(psa ~ ., data = train_data, mtry = 7, ntree = 1000, importance = TRUE)
bagged_MSE <- (val_data$psa - predict(train_bag, newdata = val_data))^2
bagged_MSEs <- c(bagged_MSEs, bagged_MSE)
train_rf <- randomForest(psa ~ ., data = train_data, mtry = round(7/3), ntree = 1000, importance = TRUE)
rf_MSE <- (val_data$psa - predict(train_rf, newdata = val_data))^2
rf_MSEs <- c(rf_MSEs, rf_MSE)
train_boost <- gbm(psa ~ ., data = train_data, distribution = "gaussian",
n.trees = 1000, interaction.depth = 1, shrinkage=0.01)
boost_MSE <- (val_data$psa - predict(train_boost, newdata = val_data, n.trees = 1000))^2
boost_MSEs <- c(boost_MSEs, boost_MSE)
}
result <- c(
mean(unpruned_MSEs),
mean(pruned_MSEs),
mean(bagged_MSEs),
mean(rf_MSEs),
mean(boost_MSEs)
)
return(setNames(result, c("unpruned_est_MSE", "pruned_est_MSE", "bagged_est_MSE", "rand_forest_est_MSE", "boosted_est_MSE")))
}
MSEs <- LOOCV_tree()
MSEs
#b
# Find optimal number of nodes via prune.tree function and cross-validation
cv.pc <- cv.tree(tree_pc, FUN = prune.tree, K=nrow(pc_data))
# Plot the deviance as a function of size
plot(cv.pc$size, cv.pc$dev, type = "b")
# Prune the tree with the elbow in mind, our elbow point is at size=4
prune.pc <- prune.tree(tree_pc, best = 4)
plot(prune.pc)
text(prune.pc, pretty = 0, cex = 0.7)
# c
bag.pc <- randomForest(psa ~ ., data = pc_data, mtry = 7, ntree = 1000, importance = TRUE)
importance(bag.pc)
varImpPlot(rf.pc)
varImpPlot(bag.pc)
# d.
# Perform random forest with the specified parameters and check importance of predictors
rf.pc <- randomForest(psa ~ ., data = pc_data, mtry = round(7/3), ntree = 1000, importance = TRUE)
rf.pc
importance(rf.pc)
varImpPlot(rf.pc)
# e.
# Perform boosting with gbm and specified parameters and check importance of predictors
boost.pc <- gbm(psa ~ ., data = pc_data, distribution = "gaussian",
n.trees = 1000, interaction.depth = 1, shrinkage=0.01)
summary(boost.pc)
MSEs
# Make a function to create different network architectures depending on parameters
# The function will also compile each network, fit the network on training data, and evaluate on testing data.
exec_network <- function(num_layers, num_units, num_epochs, dropout, L2_reg){
if(num_layers == 2){
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dense(units = 10, activation = "softmax")
}
else{
if(dropout){
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = "softmax")
}
else if(L2_reg){
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28),
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_dense(units = 10, activation = "softmax")
}
else{
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dense(units = 10, activation = "softmax")
}
}
network %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",  # loss function to minimize
metrics = c("accuracy") # monitor classification accuracy
)
history <- network %>% fit(train_images, cat_train_labels, epochs = num_epochs, batch_size = 128, verbose = F)
metrics <- network %>% evaluate(test_images, cat_test_labels, verbose = F)
return(data.frame(layers=num_layers, units=num_units, epochs=num_epochs,
dropout=dropout, L2_reg=L2_reg,
train_error=history$metrics$accuracy[num_epochs], test_error=metrics["accuracy"][[1]]))
}
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 512, num_epochs = 5, dropout = F, L2_reg = F))
# Store results in a dataframe
results_df <- data.frame(matrix(nrow=0, ncol=7))
library(keras)
# Get mnist data
mnist <- dataset_mnist()
# Partition training and test images from mnist
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
# Reshape and scale data where needed
train_images <- array_reshape(train_images, c(60000, 28*28)) # matrix
train_images <- train_images/255 # ensures all values are in [0, 1]
test_images <- array_reshape(test_images, c(10000, 28*28))
test_images <- test_images/255
# Obtain categorical versions of training and test labels
cat_train_labels <- to_categorical(train_labels)
cat_test_labels <- to_categorical(test_labels)
# Store results in a dataframe
results_df <- data.frame(matrix(nrow=0, ncol=7))
colnames(results_df) <- c("layers", "units", "epochs", "dropout", "L2_reg", "train_error", "test_error")
# Make a function to create different network architectures depending on parameters
# The function will also compile each network, fit the network on training data, and evaluate on testing data.
exec_network <- function(num_layers, num_units, num_epochs, dropout, L2_reg){
if(num_layers == 2){
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dense(units = 10, activation = "softmax")
}
else{
if(dropout){
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = "softmax")
}
else if(L2_reg){
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28),
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_dense(units = 10, activation = "softmax")
}
else{
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dense(units = 10, activation = "softmax")
}
}
network %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",  # loss function to minimize
metrics = c("accuracy") # monitor classification accuracy
)
history <- network %>% fit(train_images, cat_train_labels, epochs = num_epochs, batch_size = 128, verbose = F)
metrics <- network %>% evaluate(test_images, cat_test_labels, verbose = F)
return(data.frame(layers=num_layers, units=num_units, epochs=num_epochs,
dropout=dropout, L2_reg=L2_reg,
train_error=history$metrics$accuracy[num_epochs], test_error=metrics["accuracy"][[1]]))
}
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 512, num_epochs = 5, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 512, num_epochs = 10, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 256, num_epochs = 5, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 256, num_epochs = 10, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 2, num_units = 512, num_epochs = 5, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 2, num_units = 512, num_epochs = 10, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 2, num_units = 256, num_epochs = 5, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 2, num_units = 256, num_epochs = 10, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 512, num_epochs = 5, dropout = F, L2_reg = T))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 512, num_epochs = 5, dropout = T, L2_reg = F))
saved2_df <- results_df
View(saved2_df)
# Store results in a dataframe
results_df <- data.frame(matrix(nrow=0, ncol=7))
colnames(results_df) <- c("layers", "units", "epochs", "dropout", "L2_reg", "train_acc", "test_acc")
# Make a function to create different network architectures depending on parameters
# The function will also compile each network, fit the network on training data, and evaluate on testing data.
exec_network <- function(num_layers, num_units, num_epochs, dropout, L2_reg){
# Make models with 2 layers
if(num_layers == 2){
# Base type of model with specified number of nodes
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dense(units = 10, activation = "softmax")
}
else{
# Models with only 1 layer
if(dropout){
# Make a model with dropout
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = "softmax")
}
else if(L2_reg){
# Make a model with L2 regularization and lambda = 0.001
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28),
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_dense(units = 10, activation = "softmax")
}
else{
# Make a model without dropout or L2 regularization
network <- keras_model_sequential() %>%
layer_dense(units = num_units, activation = "relu", input_shape = c(28*28)) %>%
layer_dense(units = 10, activation = "softmax")
}
}
# Compile the network
network %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",  # loss function to minimize
metrics = c("accuracy") # monitor classification accuracy
)
# Fit the network on the training data and categorical train labels.
# This is where number of epochs parameters is passed to the fitting function
history <- network %>% fit(train_images, cat_train_labels, epochs = num_epochs, batch_size = 128, verbose = F)
# Evaluate the network on test images and the categorical test labels
metrics <- network %>% evaluate(test_images, cat_test_labels, verbose = F)
# Use metrics to report on test accuracy and history to report on training accuracy
return(data.frame(layers=num_layers, units=num_units, epochs=num_epochs,
dropout=dropout, L2_reg=L2_reg,
train_acc=history$metrics$accuracy[num_epochs], test_acc=metrics["accuracy"][[1]]))
}
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 512, num_epochs = 5, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 512, num_epochs = 10, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 256, num_epochs = 5, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 256, num_epochs = 10, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 2, num_units = 512, num_epochs = 5, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 2, num_units = 512, num_epochs = 10, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 2, num_units = 256, num_epochs = 5, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 2, num_units = 256, num_epochs = 10, dropout = F, L2_reg = F))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 512, num_epochs = 5, dropout = F, L2_reg = T))
results_df <- rbind(results_df, exec_network(num_layers = 1, num_units = 512, num_epochs = 5, dropout = T, L2_reg = F))
View(results_df)
library(keras)
# Obtain boston dataset information
boston <- dataset_boston_housing()
# Separate boston dataset into train and test data
c(c(train_data, train_targets), c(test_data, test_targets)) %<-% boston
# Obtain mean, stdev, and scale the data
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
train_data <- scale(train_data, center = mean, scale = std)
test_data <- scale(test_data, center = mean, scale = std)
# Specify a function to create a 2 hidden layer model with 64 hidden units
# using ReLU activation and linear 1-node output
build_model <- function(){
# specify the model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
# compile the model
model %>% compile(
optimizer = "rmsprop",
loss = "mse",
metrics = c("mae") # mean absolute error
)
}
# K-fold CV
# Specify 4 folds
k <- 4
# Partition indices and determine folds
indices <- sample(1:nrow(train_data))
folds <- cut(indices, breaks = k, labels = FALSE)
# Supply num epochs and create variable to track histories
num_epochs <- 200
all_mae_histories <- c()
# K-fold CV
# Specify 4 folds
k <- 4
# Partition indices and determine folds
indices <- sample(1:nrow(train_data))
folds <- cut(indices, breaks = k, labels = FALSE)
# Supply num epochs and create variable to track histories
num_epochs <- 200
all_mae_histories <- c()
for (i in 1:k){
cat("Processing fold #", i, "\n")
# Partition data into validation and training data
val_indices <- which(folds == i, arr.ind = TRUE) # prepares the validation data: data from partition #k
val_data <- train_data[val_indices,]
val_targets <- train_targets[val_indices]
partial_train_data <- train_data[-val_indices,] # prepares the training data: data from all other partitions
partial_train_targets <- train_targets[-val_indices]
# Use build model function to make architecture and compile
model <- build_model()
# Fit and track history on partial training data
history <- model %>% fit(partial_train_data, partial_train_targets,
validation_data = list(val_data, val_targets),
epochs = num_epochs, batch_size = 16,
verbose = 0) # trains the model in silent mode (verbose = 0)
# Obtain validation MAE from history
mae_history <- history$metrics$val_mae
# Store the MAE data
all_mae_histories <- rbind(all_mae_histories, mae_history)
}
# Show MAE per epoch
average_mae_history <- data.frame(
epoch = seq(1:ncol(all_mae_histories)),
validation_mae = apply(all_mae_histories, 2, mean)
)
# Plot validation MAE against epoch.
# We can see from the results that there is not much improvement after epoch 75
plot(validation_mae ~ epoch, average_mae_history, ylim = c(2, 5), type ="l")
# Repeat previous process of 4-fold CV for 2-layer, 64 hidden unit model with early stopping
all_scores <- c()
for (i in 1:k){
cat("Processing fold #", i, "\n")
val_indices <- which(folds == i, arr.ind = TRUE) # prepares the validation data: data from partition #k
val_data <- train_data[val_indices,]
val_targets <- train_targets[val_indices]
partial_train_data <- train_data[-val_indices,] # prepares the training data: data from all other partitions
partial_train_targets <- train_targets[-val_indices]
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
# compile the model
model %>% compile(
optimizer = "rmsprop",
loss = "mse",
metrics = c("mae") # mean absolute error
)
opt_history <- model %>% fit(partial_train_data, partial_train_targets, epochs = 75,
batch_size = 16, verbose = 0)
results <- model %>% evaluate(val_data, val_targets, verbose = 0)
all_scores <- c(all_scores, results["mae"])
}
mean(all_scores)
# b.
# Repeat previous process of 4-fold CV for 1-layer, 128 hidden unit model with early stopping
all_scores <- c()
for (i in 1:k){
cat("Processing fold #", i, "\n")
val_indices <- which(folds == i, arr.ind = TRUE) # prepares the validation data: data from partition #k
val_data <- train_data[val_indices,]
val_targets <- train_targets[val_indices]
partial_train_data <- train_data[-val_indices,] # prepares the training data: data from all other partitions
partial_train_targets <- train_targets[-val_indices]
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = "relu",
input_shape = dim(train_data)[2]) %>%
layer_dense(units = 1)
# compile the model
model %>% compile(
optimizer = "rmsprop",
loss = "mse",
metrics = c("mae") # mean absolute error
)
opt_history <- model %>% fit(partial_train_data, partial_train_targets, epochs = 75,
batch_size = 16, verbose = 0)
results <- model %>% evaluate(val_data, val_targets, verbose = 0)
all_scores <- c(all_scores, results["mae"])
}
mean(all_scores)
# c.
# Repeat previous process of 4-fold CV for 2-layer, 64 hidden unit model with early stopping
# with L2 regularization
all_scores <- c()
for (i in 1:k){
cat("Processing fold #", i, "\n")
val_indices <- which(folds == i, arr.ind = TRUE) # prepares the validation data: data from partition #k
val_data <- train_data[val_indices,]
val_targets <- train_targets[val_indices]
partial_train_data <- train_data[-val_indices,] # prepares the training data: data from all other partitions
partial_train_targets <- train_targets[-val_indices]
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2],
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_dense(units = 64, activation = "relu",
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_dense(units = 1)
# compile the model
model %>% compile(
optimizer = "rmsprop",
loss = "mse",
metrics = c("mae") # mean absolute error
)
opt_history <- model %>% fit(partial_train_data, partial_train_targets, epochs = 75,
batch_size = 16, verbose = 0)
results <- model %>% evaluate(val_data, val_targets, verbose = 0)
all_scores <- c(all_scores, results["mae"])
}
mean(all_scores)
# d.
# Repeat previous process of 4-fold CV for 1-layer, 128 hidden unit model with early stopping
# with L2 regularization
all_scores <- c()
for (i in 1:k){
cat("Processing fold #", i, "\n")
val_indices <- which(folds == i, arr.ind = TRUE) # prepares the validation data: data from partition #k
val_data <- train_data[val_indices,]
val_targets <- train_targets[val_indices]
partial_train_data <- train_data[-val_indices,] # prepares the training data: data from all other partitions
partial_train_targets <- train_targets[-val_indices]
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = "relu",
input_shape = dim(train_data)[2],
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_dense(units = 1)
# compile the model
model %>% compile(
optimizer = "rmsprop",
loss = "mse",
metrics = c("mae") # mean absolute error
)
opt_history <- model %>% fit(partial_train_data, partial_train_targets, epochs = 75,
batch_size = 16, verbose = 0)
results <- model %>% evaluate(val_data, val_targets, verbose = 0)
all_scores <- c(all_scores, results["mae"])
}
mean(all_scores)
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu",
input_shape = dim(train_data)[2],
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_dense(units = 64, activation = "relu",
kernel_regularizer = regularizer_l2(0.001)) %>%
layer_dense(units = 1)
# compile the model
model %>% compile(
optimizer = "rmsprop",
loss = "mse",
metrics = c("mae") # mean absolute error
)
opt_history <- model %>% fit(train_data, train_targets, epochs = 75, batch_size = 16, verbose = 0)
results <- model %>% evaluate(test_data, test_targets, verbose = 0)
results
